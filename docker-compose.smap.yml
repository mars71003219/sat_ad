services:
  # Triton OmniAnomaly Server for SMAP
  triton-omnianomaly:
    build:
      context: .
      dockerfile: triton-omnianomaly/Dockerfile.triton-omnianomaly
    image: triton-omnianomaly:latest
    container_name: triton-omnianomaly
    shm_size: 1g
    ports:
      - "8100:8000"  # HTTP
      - "8101:8001"  # gRPC
      - "8102:8002"  # Metrics
    volumes:
      - ./triton-omnianomaly/models:/models
      - ./tranad-sim/checkpoints:/checkpoints:ro
    networks:
      - telemetry-net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # SMAP Simulator
  smap-simulator:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10-slim
        WORKDIR /app
        RUN pip install --no-cache-dir confluent-kafka numpy
        COPY smap-simulator/simulator.py .
        CMD ["python", "simulator.py", \
             "--kafka", "kafka:9092", \
             "--interval", "5", \
             "--continuous"]
    container_name: smap-simulator
    volumes:
      - ./tranad-sim/processed:/tranad-data/processed:ro
    networks:
      - telemetry-net
    restart: unless-stopped

  # SMAP Inference Trigger
  smap-inference-trigger:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10-slim
        WORKDIR /app
        COPY smap-inference-trigger/requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        COPY smap-inference-trigger/trigger.py .
        CMD ["python", "trigger.py"]
    container_name: smap-inference-trigger
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_SMAP=smap-telemetry
      - KAFKA_GROUP_ID=smap-inference-trigger
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
    networks:
      - telemetry-net
    restart: unless-stopped

  # SMAP Analysis Worker
  smap-analysis-worker:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10-slim
        WORKDIR /app
        COPY smap-analysis-worker/requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        COPY smap-analysis-worker/tasks.py .
        CMD ["celery", "-A", "tasks", "worker", \
             "--loglevel=info", \
             "--concurrency=4", \
             "--pool=prefork"]
    container_name: smap-analysis-worker
    environment:
      - CELERY_BROKER_URL=amqp://guest:guest@rabbitmq:5672//
      - TRITON_OMNIANOMALY_URL=triton-omnianomaly:8000
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_RESULTS=smap-inference-results
    networks:
      - telemetry-net
    restart: unless-stopped

  # SMAP Victoria Consumer (Results to VictoriaMetrics)
  smap-victoria-consumer:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10-slim
        WORKDIR /app
        RUN pip install --no-cache-dir aiokafka==0.11.0 aiohttp==3.11.11
        COPY smap-victoria-consumer/consumer.py .
        CMD ["python", "consumer.py"]
    container_name: smap-victoria-consumer
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC_RESULTS=smap-inference-results
      - VICTORIA_METRICS_URL=http://victoria-metrics:8428
    networks:
      - telemetry-net
    restart: unless-stopped

networks:
  telemetry-net:
    name: telemetry_anomaly_det_telemetry-net
    external: true
