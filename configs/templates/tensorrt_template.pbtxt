# TensorRT Backend Configuration Template
# ONNX → TensorRT 변환 후 사용

name: "MODEL_NAME_trt"
backend: "tensorrt"
max_batch_size: 8  # TensorRT는 배치 크기 고정 권장

# 입력 텐서
input [
  {
    name: "input_data"
    data_type: TYPE_FP32  # 또는 TYPE_FP16 (FP16 변환 시)
    dims: [ 25 ]           # TensorRT는 배치 차원 제외
  }
]

# 출력 텐서
output [
  {
    name: "reconstruction"
    data_type: TYPE_FP32
    dims: [ 25 ]
  },
  {
    name: "anomaly_score"
    data_type: TYPE_FP32
    dims: [ 1 ]
  },
  {
    name: "anomaly_detected"
    data_type: TYPE_FP32
    dims: [ 1 ]
  }
]

# TensorRT 최적화 파라미터
optimization {
  execution_accelerators {
    gpu_execution_accelerator [
      {
        name: "tensorrt"
        parameters {
          key: "precision_mode"
          value: "FP16"  # FP16, FP32, INT8
        }
        parameters {
          key: "max_workspace_size_bytes"
          value: "4294967296"  # 4GB
        }
      }
    ]
  }
}

# 인스턴스 그룹
instance_group [
  {
    count: 2           # TensorRT는 다중 인스턴스 효율적
    kind: KIND_GPU
    gpus: [ 0 ]
  }
]

# 동적 배칭
dynamic_batching {
  max_queue_delay_microseconds: 100
  preferred_batch_size: [ 4, 8 ]
}

version_policy {
  latest { num_versions: 1 }
}
