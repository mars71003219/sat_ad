#!/usr/bin/env python3
"""
ë°°ì¹˜ ì™„ë£Œ ê¸°ë°˜ ì¶”ë¡  íŠ¸ë¦¬ê±°

Kafkaì—ì„œ ìœ„ì„± ë°°ì¹˜ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ì—¬:
1. ê° ë°°ì¹˜ì˜ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ëˆ„ì 
2. is_last_record=true ë©”ì‹œì§€ë¥¼ ë°›ìœ¼ë©´ ë°°ì¹˜ ì™„ë£Œë¡œ ì¸ì‹
3. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì¶”ë¡  íŠ¸ë¦¬ê±° (window_size=30, stride=10)
"""

import os
import sys
import json
import time
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from confluent_kafka import Consumer, KafkaError, KafkaException
from celery import Celery
from shared.config.settings import settings

# Logging ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Celery ì„¤ì •
celery_app = Celery(
    "inference_trigger",
    broker=settings.CELERY_BROKER_URL,
    backend=settings.CELERY_RESULT_BACKEND
)

# Kafka ì„¤ì •
KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:9092')
KAFKA_TOPIC_TELEMETRY = os.getenv('KAFKA_TOPIC_TELEMETRY', 'satellite-telemetry')

# ì¶”ë¡  ì„¤ì •
WINDOW_SIZE = int(os.getenv('WINDOW_SIZE', '30'))  # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸°
STRIDE = int(os.getenv('STRIDE', '10'))  # ìŠ¬ë¼ì´ë”© ìŠ¤íŠ¸ë¼ì´ë“œ
FORECAST_HORIZON = int(os.getenv('FORECAST_HORIZON', '10'))  # ì˜ˆì¸¡ ìŠ¤í…

# ì„œë¸Œì‹œìŠ¤í…œë³„ íŠ¹ì§• ì •ì˜
SUBSYSTEM_FEATURES = {
    'eps': [
        'satellite_battery_voltage',
        'satellite_battery_soc',
        'satellite_battery_current',
        'satellite_battery_temp',
        'satellite_solar_panel_1_voltage',
        'satellite_solar_panel_1_current',
        'satellite_solar_panel_2_voltage',
        'satellite_solar_panel_2_current',
        'satellite_solar_panel_3_voltage',
        'satellite_solar_panel_3_current',
        'satellite_power_consumption',
        'satellite_power_generation'
    ],
    'thermal': [
        'satellite_temp_battery',
        'satellite_temp_obc',
        'satellite_temp_comm',
        'satellite_temp_payload',
        'satellite_temp_solar_panel',
        'satellite_temp_external'
    ],
    'aocs': [
        'satellite_gyro_x',
        'satellite_gyro_y',
        'satellite_gyro_z',
        'satellite_sun_angle',
        'satellite_mag_x',
        'satellite_mag_y',
        'satellite_mag_z',
        'satellite_wheel_1_rpm',
        'satellite_wheel_2_rpm',
        'satellite_wheel_3_rpm',
        'satellite_altitude',
        'satellite_velocity'
    ],
    'comm': [
        'satellite_rssi',
        'satellite_data_backlog',
        'satellite_last_contact'
    ]
}


class BatchInferenceTrigger:
    """ë°°ì¹˜ ì™„ë£Œ ê¸°ë°˜ ì¶”ë¡  íŠ¸ë¦¬ê±°"""

    def __init__(self):
        """
        ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ë°°ì¹˜ ì™„ë£Œ ì‹œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¶”ë¡ ì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
        """
        # ë°°ì¹˜ ë²„í¼: {batch_id: [record1, record2, ...]}
        self.batch_buffers = defaultdict(list)

        # ë°°ì¹˜ ë©”íƒ€ë°ì´í„°: {batch_id: metadata}
        self.batch_metadata = {}

        # Kafka Consumer ì´ˆê¸°í™”
        self.consumer = self._create_consumer()

        logger.info(f"Batch Inference Trigger initialized")
        logger.info(f"Kafka: {KAFKA_BOOTSTRAP_SERVERS}, Topic: {KAFKA_TOPIC_TELEMETRY}")
        logger.info(f"Window Size: {WINDOW_SIZE}, Stride: {STRIDE}")

    def _create_consumer(self):
        """Confluent Kafka Consumer ìƒì„±"""
        conf = {
            'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
            'group.id': 'batch-inference-trigger-group',
            'auto.offset.reset': 'latest',
            'enable.auto.commit': True,
            'session.timeout.ms': 6000,
            'max.poll.interval.ms': 300000
        }
        consumer = Consumer(conf)
        consumer.subscribe([KAFKA_TOPIC_TELEMETRY])
        return consumer

    def process_message(self, message: Dict[str, Any]):
        """
        Kafka ë©”ì‹œì§€ ì²˜ë¦¬

        Args:
            message: ë°°ì¹˜ í…”ë ˆë©”íŠ¸ë¦¬ ë©”ì‹œì§€
        """
        try:
            batch_id = message.get('batch_id')
            satellite_id = message.get('satellite_id')
            record_index = message.get('record_index')
            is_last = message.get('is_last_record', False)
            data = message.get('data', {})

            if not batch_id or not satellite_id:
                logger.warning("Missing batch_id or satellite_id in message")
                return

            # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì €ì¥ (ì²« ë©”ì‹œì§€ì¼ ë•Œ)
            if batch_id not in self.batch_metadata:
                self.batch_metadata[batch_id] = {
                    'satellite_id': satellite_id,
                    'batch_start_time': message.get('batch_start_time'),
                    'batch_end_time': message.get('batch_end_time'),
                    'total_records': message.get('total_records'),
                    'started_at': datetime.utcnow().isoformat()
                }
                logger.info(f"ğŸ“¦ New batch started: {batch_id} ({satellite_id})")

            # ë°ì´í„°ë¥¼ ë°°ì¹˜ ë²„í¼ì— ì¶”ê°€
            self.batch_buffers[batch_id].append({
                'index': record_index,
                'data': data
            })

            logger.debug(f"[{batch_id}] Received record {record_index+1}/{message.get('total_records')}")

            # ë°°ì¹˜ ì™„ë£Œ í™•ì¸
            if is_last:
                logger.info(f"âœ… Batch complete: {batch_id} - {len(self.batch_buffers[batch_id])} records")
                self.on_batch_complete(batch_id)

        except Exception as e:
            logger.error(f"Error processing message: {e}", exc_info=True)

    def on_batch_complete(self, batch_id: str):
        """
        ë°°ì¹˜ ì™„ë£Œ ì‹œ í˜¸ì¶œ - ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¶”ë¡  íŠ¸ë¦¬ê±°

        Args:
            batch_id: ì™„ë£Œëœ ë°°ì¹˜ ID
        """
        try:
            metadata = self.batch_metadata.get(batch_id)
            records = self.batch_buffers.get(batch_id, [])

            if not metadata or not records:
                logger.error(f"Batch data not found: {batch_id}")
                return

            satellite_id = metadata['satellite_id']
            total_records = len(records)

            logger.info(f"ğŸ” Processing batch {batch_id}")
            logger.info(f"  - Satellite: {satellite_id}")
            logger.info(f"  - Records: {total_records}")

            # ë ˆì½”ë“œë¥¼ ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬
            records.sort(key=lambda r: r['index'])

            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì¶”ë¡  íŠ¸ë¦¬ê±°
            num_windows = self.trigger_sliding_window_inference(batch_id, satellite_id, records)

            logger.info(f"ğŸš€ Triggered {num_windows} inference windows for batch {batch_id}")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del self.batch_buffers[batch_id]
            del self.batch_metadata[batch_id]

        except Exception as e:
            logger.error(f"Error on batch complete: {e}", exc_info=True)

    def trigger_sliding_window_inference(
        self,
        batch_id: str,
        satellite_id: str,
        records: List[Dict[str, Any]]
    ) -> int:
        """
        ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì¶”ë¡  íŠ¸ë¦¬ê±°

        Args:
            batch_id: ë°°ì¹˜ ID
            satellite_id: ìœ„ì„± ID
            records: ì •ë ¬ëœ ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸

        Returns:
            íŠ¸ë¦¬ê±°ëœ ìœˆë„ìš° ìˆ˜
        """
        total_records = len(records)
        window_count = 0

        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if total_records < WINDOW_SIZE:
            logger.warning(f"Insufficient data for inference: {total_records} < {WINDOW_SIZE}")
            return 0

        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì¶”ë¡  íŠ¸ë¦¬ê±°
        # ì˜ˆ: total=120, window=30, stride=10
        # [0:30], [10:40], [20:50], ..., [90:120]
        for start_idx in range(0, total_records - WINDOW_SIZE + 1, STRIDE):
            end_idx = start_idx + WINDOW_SIZE
            window_records = records[start_idx:end_idx]

            # ê° ì„œë¸Œì‹œìŠ¤í…œì— ëŒ€í•´ ì¶”ë¡  íŠ¸ë¦¬ê±°
            for subsystem in SUBSYSTEM_FEATURES.keys():
                success = self.trigger_subsystem_inference(
                    batch_id=batch_id,
                    satellite_id=satellite_id,
                    subsystem=subsystem,
                    window_records=window_records,
                    window_index=window_count
                )

                if success:
                    window_count += 1

        return window_count

    def trigger_subsystem_inference(
        self,
        batch_id: str,
        satellite_id: str,
        subsystem: str,
        window_records: List[Dict[str, Any]],
        window_index: int
    ) -> bool:
        """
        ì„œë¸Œì‹œìŠ¤í…œ ì¶”ë¡  íŠ¸ë¦¬ê±°

        Args:
            batch_id: ë°°ì¹˜ ID
            satellite_id: ìœ„ì„± ID
            subsystem: ì„œë¸Œì‹œìŠ¤í…œ ì´ë¦„
            window_records: ìœˆë„ìš° ë ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸
            window_index: ìœˆë„ìš° ì¸ë±ìŠ¤

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # íŠ¹ì§• ì¶”ì¶œ - [sequence_length, features] í˜•íƒœë¡œ êµ¬ì„±
            required_features = SUBSYSTEM_FEATURES.get(subsystem, [])
            input_data = []  # [[feat1_t1, feat2_t1, ...], [feat1_t2, feat2_t2, ...], ...]
            input_features = required_features

            for record in window_records:
                data = record['data']
                record_features = []

                for feature in required_features:
                    value = data.get(feature)
                    if value is None:
                        logger.warning(f"Missing feature {feature} in window")
                        return False
                    record_features.append(value)

                input_data.append(record_features)

            # Celery íƒœìŠ¤í¬ ìƒì„±
            job_id = f"batch-{batch_id}-{subsystem}-win{window_index}-{int(time.time())}"

            task_params = {
                'job_id': job_id,
                'subsystem': subsystem,
                'model_name': f'transformer_{subsystem}',  # Triton ëª¨ë¸ ì´ë¦„ê³¼ ì¼ì¹˜
                'input_data': input_data,
                'input_features': input_features,
                'config': {
                    'sequence_length': WINDOW_SIZE,
                    'forecast_horizon': FORECAST_HORIZON
                },
                'metadata': {
                    'satellite_id': satellite_id,
                    'batch_id': batch_id,
                    'window_index': window_index,
                    'source': 'batch_trigger',
                    'trigger_reason': 'batch_complete_sliding_window',
                    'created_at': datetime.utcnow().isoformat()
                }
            }

            # Celery íƒœìŠ¤í¬ ì „ì†¡
            celery_app.send_task(
                'analysis_server.tasks.run_subsystem_inference',
                kwargs=task_params,
                queue='inference'
            )

            logger.debug(f"Triggered inference: {job_id}")
            return True

        except Exception as e:
            logger.error(f"Error triggering inference for {subsystem}: {e}", exc_info=True)
            return False

    def run(self):
        """ë©”ì¸ ë£¨í”„: Kafka ë©”ì‹œì§€ ì²˜ë¦¬"""
        logger.info("=" * 60)
        logger.info("Batch-based Inference Trigger Starting...")
        logger.info("=" * 60)

        retry_count = 0
        max_retries = 10

        while retry_count < max_retries:
            try:
                while True:
                    msg = self.consumer.poll(timeout=1.0)

                    if msg is None:
                        continue

                    if msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            continue
                        else:
                            logger.error(f"Consumer error: {msg.error()}")
                            raise KafkaException(msg.error())

                    try:
                        data = json.loads(msg.value().decode('utf-8'))
                        self.process_message(data)
                        retry_count = 0  # ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ë¦¬ì…‹

                    except Exception as e:
                        logger.error(f"Error processing Kafka message: {e}", exc_info=True)
                        continue

            except KeyboardInterrupt:
                logger.info("Shutting down gracefully...")
                break
            except Exception as e:
                retry_count += 1
                logger.error(f"Error in main loop (retry {retry_count}/{max_retries}): {e}", exc_info=True)
                if retry_count < max_retries:
                    sleep_time = min(5 * retry_count, 30)  # ìµœëŒ€ 30ì´ˆ
                    logger.info(f"Retrying in {sleep_time} seconds...")
                    time.sleep(sleep_time)
                    # ì»¨ìŠˆë¨¸ ì¬ìƒì„±
                    try:
                        self.consumer.close()
                    except:
                        pass
                    self.consumer = self._create_consumer()
                else:
                    logger.error("Max retries reached, exiting...")
                    break

        try:
            self.consumer.close()
            logger.info("Kafka consumer closed")
        except:
            pass


if __name__ == '__main__':
    trigger = BatchInferenceTrigger()
    trigger.run()
